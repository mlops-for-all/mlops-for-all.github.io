{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "\n",
    "ROOT_PATH = Path(\"python/translation/main.py\").parent\n",
    "OPENAI_ENV_PATH = ROOT_PATH.parent / \"env\" / \"openai.env\"\n",
    "dotenv.load_dotenv(\"/Users/aiden/workspace/mlops-for-all/mlops-for-all.github.io/python/env/openai.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}. \"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "    input_variables=[\"input_language\", \"output_language\"],\n",
    ")\n",
    "system_message_prompt_2 = SystemMessagePromptTemplate(prompt=prompt)\n",
    "\n",
    "assert system_message_prompt == system_message_prompt_2\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Summary of how Seldon Core creates an API server: \\n\\n1. initContainer downloads the necessary model from the model repository. \\n2. The downloaded model is passed to the container. \\n3. The container runs an API server that wraps the model. \\n4. The API can be requested at the generated API server address to receive the inference values of the model. \\n\\nSeldonDeployment spec, which is the custom resource used most often when using Seldon Core: \\n\\nThe yaml file that defines the SeldonDeployment custom resource is as follows:', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = llm.predict_messages([HumanMessage(content=f\"Translate this sentence from Korean to English. {source_sentence}\")])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of how Seldon Core creates an API server: \n",
      "\n",
      "1. initContainer downloads the necessary model from the model repository. \n",
      "2. The downloaded model is passed to the container. \n",
      "3. The container runs an API server that wraps the model. \n",
      "4. The API can be requested at the generated API server address to receive the inference values of the model. \n",
      "\n",
      "SeldonDeployment spec, which is the custom resource used most often when using Seldon Core: \n",
      "\n",
      "The yaml file that defines the SeldonDeployment custom resource is as follows:\n"
     ]
    }
   ],
   "source": [
    "print(pred.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## How Seldon Core works?\n",
      "\n",
      "Seldon Core가 API 서버를 생성하는 과정을 요약하면 다음과 같습니다.\n",
      "\n",
      "![seldon-fields-0.png](./img/seldon-fields-0.png)\n",
      "\n",
      "1. initContainer는 모델 저장소에서 필요한 모델을 다운로드 받습니다.\n",
      "2. 다운로드받은 모델을 container로 전달합니다.\n",
      "3. container는 전달받은 모델을 감싼 API 서버를 실행합니다.\n",
      "4. 생성된 API 서버 주소로 API를 요청하여 모델의 추론 값을 받을 수 있습니다.\n",
      "\n",
      "## SeldonDeployment Spec\n",
      "\n",
      "Seldon Core를 사용할 때, 주로 사용하게 되는 커스텀 리소스인 SeldonDeployment를 정의하는 yaml 파일은 다음과 같습니다.\n",
      "\n",
      "\n",
      "\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}), HumanMessage(content='\\n## How Seldon Core works?\\n\\nSeldon Core가 API 서버를 생성하는 과정을 요약하면 다음과 같습니다.\\n\\n![seldon-fields-0.png](./img/seldon-fields-0.png)\\n\\n1. initContainer는 모델 저장소에서 필요한 모델을 다운로드 받습니다.\\n2. 다운로드받은 모델을 container로 전달합니다.\\n3. container는 전달받은 모델을 감싼 API 서버를 실행합니다.\\n4. 생성된 API 서버 주소로 API를 요청하여 모델의 추론 값을 받을 수 있습니다.\\n\\n## SeldonDeployment Spec\\n\\nSeldon Core를 사용할 때, 주로 사용하게 되는 커스텀 리소스인 SeldonDeployment를 정의하는 yaml 파일은 다음과 같습니다.\\n\\n\\n', additional_kwargs={}, example=False)]\n",
      "content='```yaml\\napiVersion: machinelearning.seldon.io/v1alpha2\\nkind: SeldonDeployment\\nmetadata:\\n  name: seldon-model-deployment\\nspec:\\n  name: seldon-model-deployment\\n  predictors:\\n  - componentSpecs:\\n    - spec:\\n        containers:\\n        - image: <model-image>\\n        name: <model-name>\\n    graph:\\n      children: []\\n      endpoint:\\n        type: REST\\n      name: <model-name>\\n    name: <predictor-name>\\n    replicas: 1\\n```' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DOCS_ROOT = ROOT_PATH.parent.parent\n",
    "source_path = \"/Users/aiden/workspace/mlops-for-all/mlops-for-all.github.io/i18n/en/docusaurus-plugin-content-docs/version-1.0/api-deployment/seldon-fields.md\"\n",
    "\n",
    "translate_lines = []\n",
    "with open(source_path, \"r\") as f:\n",
    "    line = f.readline()\n",
    "    translate_lines += [line]\n",
    "    lines = []\n",
    "    is_codeblock = False\n",
    "    is_header = True\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "        # 헤더 블록인 경우\n",
    "        if line.startswith(\"---\"):\n",
    "            is_header = False\n",
    "            translate_lines += [line]\n",
    "            continue\n",
    "        if is_header:\n",
    "            translate_lines += [line]\n",
    "            continue\n",
    "\n",
    "        # 코드 블록인 경우\n",
    "        if line.startswith(\"```\"):\n",
    "            # 첫 번째 코드 블록인 경우 번역한다.\n",
    "            if not is_codeblock:\n",
    "                source_sentence = \"\".join(lines)\n",
    "                if source_sentence:\n",
    "                    #\n",
    "                    # request\n",
    "                    print(source_sentence)\n",
    "                    translation_prompt = chat_prompt.format_prompt(input_language=\"English\", output_language=\"Korean\", text=source_sentence).to_messages()\n",
    "                    print(translation_prompt)\n",
    "                    # translated_sentence = chain.run(source_sentence)\n",
    "                    translated_sentence = llm.predict_messages(translation_prompt)\n",
    "                    print(translated_sentence)\n",
    "                    #\n",
    "                    translate_lines += [translated_sentence]\n",
    "                    translate_lines += [\"\\n\"]\n",
    "                    break\n",
    "                else:\n",
    "                    translate_lines += [\"\\n\"]\n",
    "                lines = []\n",
    "                is_codeblock = True\n",
    "            else:\n",
    "                is_codeblock = False\n",
    "                translate_lines += [line]\n",
    "                continue\n",
    "        if is_codeblock:\n",
    "            # 코드 블록 내부인 경우 통과한다.\n",
    "            translate_lines += [line]\n",
    "            continue\n",
    "        lines += [line]\n",
    "\n",
    "# source_sentence = \"\".join(lines)\n",
    "# #\n",
    "# # request\n",
    "# translation_prompt = chat_prompt.format_prompt(input_language=\"English\", output_language=\"Korean\", text=source_sentence).to_messages()\n",
    "# translated_sentence = llm.predict_messages(translation_prompt)\n",
    "# #\n",
    "# translate_lines += [translated_sentence]\n",
    "# translate_lines += [\"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}),\n",
       " HumanMessage(content='\\n## How Seldon Core works?\\n\\nSeldon Core가 API 서버를 생성하는 과정을 요약하면 다음과 같습니다.\\n\\n![seldon-fields-0.png](./img/seldon-fields-0.png)\\n\\n1. initContainer는 모델 저장소에서 필요한 모델을 다운로드 받습니다.\\n2. 다운로드받은 모델을 container로 전달합니다.\\n3. container는 전달받은 모델을 감싼 API 서버를 실행합니다.\\n4. 생성된 API 서버 주소로 API를 요청하여 모델의 추론 값을 받을 수 있습니다.\\n\\n## SeldonDeployment Spec\\n\\nSeldon Core를 사용할 때, 주로 사용하게 되는 커스텀 리소스인 SeldonDeployment를 정의하는 yaml 파일은 다음과 같습니다.\\n\\n\\n', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='``` yaml\\napiVersion: machinelearning.seldon.io/v1alpha2\\nkind: SeldonDeployment\\nmetadata:\\n  name: seldon-model\\nspec:\\n  name: test-deployment\\n  predictors:\\n  - graph:\\n      name: mymodel\\n      type: MODEL\\n      modelUri: gs://my-bucket/model.pkl\\n      endpoint:\\n      - type: REST\\n        servicePort: 80\\n```\\n\\n이 yaml 파일은 SeldonDeployment를 정의하고 모델이 저장된 저장소의 위치를 지정하고, Seldon Core가 생성할 API 서버의 포트번호를 지정합니다.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_messages(translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "apiVersion: machinelearning.seldon.io/v1alpha2\n",
      "kind: SeldonDeployment\n",
      "metadata:\n",
      "  name: <커스텀 리소스 이름>\n",
      "spec:\n",
      "  name: <모델 이름>\n",
      "  predictors:\n",
      "  - componentSpecs:\n",
      "    - spec:\n",
      "        containers:\n",
      "        - image: <컨테이너 이미지>\n",
      "    graph:\n",
      "      children: []\n",
      "      endpoint:\n",
      "        type: REST\n",
      "      name: <모델 이름>\n",
      "      type: MODEL\n",
      "```\n",
      "\n",
      "SeldonDeployment는 모델을 배포하기 위해, 컨테이너 이미지, 모델 이름, 등의 리소스\n"
     ]
    }
   ],
   "source": [
    "print(translated_sentence.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs = \"\".join(translate_lines)\n",
    "print(docs)\n",
    "dest_path = DOCS_ROOT/ \"i18n/en/docusaurus-plugin-content-docs/version-1.0/api-deployment\" / \"seldon-fields.md\"\n",
    "with open(dest_path, \"w\") as f:\n",
    "    f.write(docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.9.15",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
