"use strict";(self.webpackChunkv_2=self.webpackChunkv_2||[]).push([[8367],{3905:(e,n,a)=>{a.d(n,{Zo:()=>d,kt:()=>c});var t=a(7294);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function l(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function i(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?l(Object(a),!0).forEach((function(n){r(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function o(e,n){if(null==e)return{};var a,t,r=function(e,n){if(null==e)return{};var a,t,r={},l=Object.keys(e);for(t=0;t<l.length;t++)a=l[t],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(t=0;t<l.length;t++)a=l[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=t.createContext({}),s=function(e){var n=t.useContext(p),a=n;return e&&(a="function"==typeof e?e(n):i(i({},n),e)),a},d=function(e){var n=s(e.components);return t.createElement(p.Provider,{value:n},e.children)},m="mdxType",_={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},u=t.forwardRef((function(e,n){var a=e.components,r=e.mdxType,l=e.originalType,p=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),m=s(a),u=r,c=m["".concat(p,".").concat(u)]||m[u]||_[u]||l;return a?t.createElement(c,i(i({ref:n},d),{},{components:a})):t.createElement(c,i({ref:n},d))}));function c(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var l=a.length,i=new Array(l);i[0]=u;var o={};for(var p in n)hasOwnProperty.call(n,p)&&(o[p]=n[p]);o.originalType=e,o[m]="string"==typeof e?e:r,i[1]=o;for(var s=2;s<l;s++)i[s]=a[s];return t.createElement.apply(null,i)}return t.createElement.apply(null,a)}u.displayName="MDXCreateElement"},7871:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>i,default:()=>_,frontMatter:()=>l,metadata:()=>o,toc:()=>s});var t=a(7462),r=(a(7294),a(3905));const l={title:"12. Component - MLFlow",description:"",sidebar_position:12,date:new Date("2021-12-13T00:00:00.000Z"),lastmod:new Date("2021-12-20T00:00:00.000Z"),contributors:["Jongseob Jeon","SeungTae Kim"]},i=void 0,o={unversionedId:"kubeflow/advanced-mlflow",id:"kubeflow/advanced-mlflow",title:"12. Component - MLFlow",description:"",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/kubeflow/advanced-mlflow.md",sourceDirName:"kubeflow",slug:"/kubeflow/advanced-mlflow",permalink:"/en/docs/kubeflow/advanced-mlflow",draft:!1,editUrl:"https://github.com/mlops-for-all/mlops-for-all.github.io/tree/main/docs/kubeflow/advanced-mlflow.md",tags:[],version:"current",lastUpdatedBy:"Tim cho",lastUpdatedAt:1708479021,formattedLastUpdatedAt:"Feb 21, 2024",sidebarPosition:12,frontMatter:{title:"12. Component - MLFlow",description:"",sidebar_position:12,date:"2021-12-13T00:00:00.000Z",lastmod:"2021-12-20T00:00:00.000Z",contributors:["Jongseob Jeon","SeungTae Kim"]},sidebar:"tutorialSidebar",previous:{title:"11. Pipeline - Run Result",permalink:"/en/docs/kubeflow/advanced-run"},next:{title:"13. Component - Debugging",permalink:"/en/docs/kubeflow/how-to-debug"}},p={},s=[{value:"MLFlow Component",id:"mlflow-component",level:2},{value:"MLFlow in Local",id:"mlflow-in-local",level:2},{value:"1. Train model",id:"1-train-model",level:3},{value:"2. MLFLow Infos",id:"2-mlflow-infos",level:3},{value:"3. Save MLFLow Infos",id:"3-save-mlflow-infos",level:3},{value:"MLFlow on Server",id:"mlflow-on-server",level:2},{value:"MLFlow Component",id:"mlflow-component-1",level:2},{value:"MLFlow Pipeline",id:"mlflow-pipeline",level:2},{value:"Data Component",id:"data-component",level:3},{value:"Pipeline",id:"pipeline",level:3},{value:"Run",id:"run",level:3}],d={toc:s},m="wrapper";function _(e){let{components:n,...l}=e;return(0,r.kt)(m,(0,t.Z)({},d,l,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"mlflow-component"},"MLFlow Component"),(0,r.kt)("p",null,"In this page, we will explain the process of writing a component to store the model in MLFlow so that the model trained in ",(0,r.kt)("a",{parentName:"p",href:"/en/docs/kubeflow/advanced-component"},"Advanced Usage Component")," can be linked to API deployment."),(0,r.kt)("h2",{id:"mlflow-in-local"},"MLFlow in Local"),(0,r.kt)("p",null,"In order to store the model in MLFlow and use it in serving, the following items are needed."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"model"),(0,r.kt)("li",{parentName:"ul"},"signature"),(0,r.kt)("li",{parentName:"ul"},"input_example"),(0,r.kt)("li",{parentName:"ul"},"conda_env")),(0,r.kt)("p",null,"We will look into the process of saving a model to MLFlow through Python code."),(0,r.kt)("h3",{id:"1-train-model"},"1. Train model"),(0,r.kt)("p",null,"The following steps involve training an SVC model using the iris dataset."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\n\niris = load_iris()\n\ndata = pd.DataFrame(iris["data"], columns=iris["feature_names"])\ntarget = pd.DataFrame(iris["target"], columns=["target"])\n\nclf = SVC(kernel="rbf")\nclf.fit(data, target)\n\n')),(0,r.kt)("h3",{id:"2-mlflow-infos"},"2. MLFLow Infos"),(0,r.kt)("p",null,"This process creates the necessary information for MLFlow."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from mlflow.models.signature import infer_signature\nfrom mlflow.utils.environment import _mlflow_conda_env\n\ninput_example = data.sample(1)\nsignature = infer_signature(data, clf.predict(data))\nconda_env = _mlflow_conda_env(additional_pip_deps=["dill", "pandas", "scikit-learn"])\n')),(0,r.kt)("p",null,"Each variable's content is as follows."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"input_example")),(0,r.kt)("table",{parentName:"li"},(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"sepal length (cm)"),(0,r.kt)("th",{parentName:"tr",align:null},"sepal width (cm)"),(0,r.kt)("th",{parentName:"tr",align:null},"petal length (cm)"),(0,r.kt)("th",{parentName:"tr",align:null},"petal width (cm)"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"6.5"),(0,r.kt)("td",{parentName:"tr",align:null},"6.7"),(0,r.kt)("td",{parentName:"tr",align:null},"3.1"),(0,r.kt)("td",{parentName:"tr",align:null},"4.4"))))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"signature")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-python"},"inputs:\n  ['sepal length (cm)': double, 'sepal width (cm)': double, 'petal length (cm)': double, 'petal width (cm)': double]\noutputs:\n  [Tensor('int64', (-1,))]\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("inlineCode",{parentName:"p"},"conda_env")),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-python"},"{'name': 'mlflow-env',\n 'channels': ['conda-forge'],\n 'dependencies': ['python=3.8.10',\n  'pip',\n  {'pip': ['mlflow', 'dill', 'pandas', 'scikit-learn']}]}\n")))),(0,r.kt)("h3",{id:"3-save-mlflow-infos"},"3. Save MLFLow Infos"),(0,r.kt)("p",null,"Next, we save the learned information and the model. Since the trained model uses the sklearn package, we can easily save the model using ",(0,r.kt)("inlineCode",{parentName:"p"},"mlflow.sklearn"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from mlflow.sklearn import save_model\n\nsave_model(\n    sk_model=clf,\n    path="svc",\n    serialization_format="cloudpickle",\n    conda_env=conda_env,\n    signature=signature,\n    input_example=input_example,\n)\n')),(0,r.kt)("p",null,"If you work locally, a svc folder will be created and the following files will be generated."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"ls svc\n")),(0,r.kt)("p",null,"If you execute the command above, you can check the following output value."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"MLmodel            conda.yaml         input_example.json model.pkl          requirements.txt\n")),(0,r.kt)("p",null,"Each file will be as follows if checked."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"MLmodel"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'flavors:\n  python_function:\n    env: conda.yaml\n    loader_module: mlflow.sklearn\n    model_path: model.pkl\n    python_version: 3.8.10\n  sklearn:\n    pickled_model: model.pkl\n    serialization_format: cloudpickle\n    sklearn_version: 1.0.1\nsaved_input_example_info:\n  artifact_path: input_example.json\n  pandas_orient: split\n  type: dataframe\nsignature:\n  inputs: \'[{"name": "sepal length (cm)", "type": "double"}, {"name": "sepal width\n    (cm)", "type": "double"}, {"name": "petal length (cm)", "type": "double"}, {"name":\n    "petal width (cm)", "type": "double"}]\'\n  outputs: \'[{"type": "tensor", "tensor-spec": {"dtype": "int64", "shape": [-1]}}]\'\nutc_time_created: \'2021-12-06 06:52:30.612810\'\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"conda.yaml"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"channels:\n- conda-forge\ndependencies:\n- python=3.8.10\n- pip\n- pip:\n  - mlflow\n  - dill\n  - pandas\n  - scikit-learn\nname: mlflow-env\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"input_example.json"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'{\n    "columns": \n    [\n        "sepal length (cm)",\n        "sepal width (cm)",\n        "petal length (cm)",\n        "petal width (cm)"\n    ],\n    "data": \n    [\n        [6.7, 3.1, 4.4, 1.4]\n    ]\n}\n'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"requirements.txt"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"mlflow\ndill\npandas\nscikit-learn\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"model.pkl"))),(0,r.kt)("h2",{id:"mlflow-on-server"},"MLFlow on Server"),(0,r.kt)("p",null,"Now, let's proceed with the task of uploading the saved model to the MLflow server."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'import mlflow\n\nwith mlflow.start_run():\n    mlflow.log_artifact("svc/")\n')),(0,r.kt)("p",null,"Save and open the ",(0,r.kt)("inlineCode",{parentName:"p"},"mlruns")," directory generated path with ",(0,r.kt)("inlineCode",{parentName:"p"},"mlflow ui")," command to launch mlflow server and dashboard.\nAccess the mlflow dashboard, click the generated run to view it as below."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"mlflow-0.png",src:a(8730).Z,width:"2782",height:"2496"}),"\n(This screen may vary depending on the version of mlflow.)"),(0,r.kt)("h2",{id:"mlflow-component-1"},"MLFlow Component"),(0,r.kt)("p",null,"Now, let's write a reusable component in Kubeflow."),(0,r.kt)("p",null,"The ways of writing components that can be reused are broadly divided into three categories."),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"After saving the necessary environment in the component responsible for model training, the MLflow component is only responsible for the upload."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"mlflow-1.png",src:a(6694).Z,width:"578",height:"844"}))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Pass the trained model and data to the MLflow component, which is responsible for saving and uploading."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"mlflow-2.png",src:a(5944).Z,width:"900",height:"846"}))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"The component responsible for model training handles both saving and uploading."),(0,r.kt)("p",{parentName:"li"},(0,r.kt)("img",{alt:"mlflow-3.png",src:a(5109).Z,width:"578",height:"406"})))),(0,r.kt)("p",null,"We are trying to manage the model through the first approach.\nThe reason is that we don't need to write the code to upload the MLFlow model every time like three times for each component written."),(0,r.kt)("p",null,"Reusing components is possible by the methods 1 and 2.\nHowever, in the case of 2, it is necessary to deliver the trained image and packages to the component, so ultimately additional information about the component must be delivered."),(0,r.kt)("p",null,"In order to proceed with the method 1, the learning component must also be changed.\nCode that stores the environment needed to save the model must be added."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from functools import partial\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n@partial(\n    create_component_from_func,\n    packages_to_install=["dill", "pandas", "scikit-learn", "mlflow"],\n)\ndef train_from_csv(\n    train_data_path: InputPath("csv"),\n    train_target_path: InputPath("csv"),\n    model_path: OutputPath("dill"),\n    input_example_path: OutputPath("dill"),\n    signature_path: OutputPath("dill"),\n    conda_env_path: OutputPath("dill"),\n    kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode="wb") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, "wb") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, "wb") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=["dill", "pandas", "scikit-learn"]\n    )\n    with open(conda_env_path, "wb") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n')),(0,r.kt)("p",null,"Write a component to upload to MLFlow.\nAt this time, configure the uploaded MLFlow endpoint to be connected to the ",(0,r.kt)("a",{parentName:"p",href:"/en/docs/setup-components/install-components-mlflow"},"mlflow service")," that we installed.",(0,r.kt)("br",{parentName:"p"}),"\n","In this case, use the Kubernetes Service DNS Name of the Minio installed at the time of MLFlow Server installation. As this service is created in the Kubeflow namespace with the name minio-service, set it to ",(0,r.kt)("inlineCode",{parentName:"p"},"http://minio-service.kubeflow.svc:9000"),".",(0,r.kt)("br",{parentName:"p"}),"\n","Similarly, for the tracking_uri address, use the Kubernetes Service DNS Name of the MLFlow server and set it to ",(0,r.kt)("inlineCode",{parentName:"p"},"http://mlflow-server-service.mlflow-system.svc:5000"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from functools import partial\nfrom kfp.components import InputPath, create_component_from_func\n\n@partial(\n    create_component_from_func,\n    packages_to_install=["dill", "pandas", "scikit-learn", "mlflow", "boto3"],\n)\ndef upload_sklearn_model_to_mlflow(\n    model_name: str,\n    model_path: InputPath("dill"),\n    input_example_path: InputPath("dill"),\n    signature_path: InputPath("dill"),\n    conda_env_path: InputPath("dill"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n    \n    from mlflow.tracking.client import MlflowClient\n\n    os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://minio-service.kubeflow.svc:9000"\n    os.environ["AWS_ACCESS_KEY_ID"] = "minio"\n    os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"\n\n    client = MlflowClient("http://mlflow-server-service.mlflow-system.svc:5000")\n\n    with open(model_path, mode="rb") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, "rb") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, "rb") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, "rb") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format="cloudpickle",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id="0")\n    client.log_artifact(run.info.run_id, model_name)\n')),(0,r.kt)("h2",{id:"mlflow-pipeline"},"MLFlow Pipeline"),(0,r.kt)("p",null,"Now let's connect the components we have written and create a pipeline. "),(0,r.kt)("h3",{id:"data-component"},"Data Component"),(0,r.kt)("p",null,"The data we will use to train the model is sklearn's iris.\nWe will write a component to generate the data."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from functools import partial\n\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=["pandas", "scikit-learn"],\n)\ndef load_iris_data(\n    data_path: OutputPath("csv"),\n    target_path: OutputPath("csv"),\n):\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris["data"], columns=iris["feature_names"])\n    target = pd.DataFrame(iris["target"], columns=["target"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n')),(0,r.kt)("h3",{id:"pipeline"},"Pipeline"),(0,r.kt)("p",null,"The pipeline code can be written as follows."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from kfp.dsl import pipeline\n\n\n@pipeline(name="mlflow_pipeline")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs["data"],\n        train_target=iris_data.outputs["target"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs["model"],\n        input_example=model.outputs["input_example"],\n        signature=model.outputs["signature"],\n        conda_env=model.outputs["conda_env"],\n    )\n')),(0,r.kt)("h3",{id:"run"},"Run"),(0,r.kt)("p",null,"If you organize the components and pipelines written above into a single Python file, it would look like this."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from functools import partial\n\nimport kfp\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\nfrom kfp.dsl import pipeline\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=["pandas", "scikit-learn"],\n)\ndef load_iris_data(\n    data_path: OutputPath("csv"),\n    target_path: OutputPath("csv"),\n):\n    import pandas as pd\n    from sklearn.datasets import load_iris\n\n    iris = load_iris()\n\n    data = pd.DataFrame(iris["data"], columns=iris["feature_names"])\n    target = pd.DataFrame(iris["target"], columns=["target"])\n\n    data.to_csv(data_path, index=False)\n    target.to_csv(target_path, index=False)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=["dill", "pandas", "scikit-learn", "mlflow"],\n)\ndef train_from_csv(\n    train_data_path: InputPath("csv"),\n    train_target_path: InputPath("csv"),\n    model_path: OutputPath("dill"),\n    input_example_path: OutputPath("dill"),\n    signature_path: OutputPath("dill"),\n    conda_env_path: OutputPath("dill"),\n    kernel: str,\n):\n    import dill\n    import pandas as pd\n    from sklearn.svm import SVC\n\n    from mlflow.models.signature import infer_signature\n    from mlflow.utils.environment import _mlflow_conda_env\n\n    train_data = pd.read_csv(train_data_path)\n    train_target = pd.read_csv(train_target_path)\n\n    clf = SVC(kernel=kernel)\n    clf.fit(train_data, train_target)\n\n    with open(model_path, mode="wb") as file_writer:\n        dill.dump(clf, file_writer)\n\n    input_example = train_data.sample(1)\n    with open(input_example_path, "wb") as file_writer:\n        dill.dump(input_example, file_writer)\n\n    signature = infer_signature(train_data, clf.predict(train_data))\n    with open(signature_path, "wb") as file_writer:\n        dill.dump(signature, file_writer)\n\n    conda_env = _mlflow_conda_env(\n        additional_pip_deps=["dill", "pandas", "scikit-learn"]\n    )\n    with open(conda_env_path, "wb") as file_writer:\n        dill.dump(conda_env, file_writer)\n\n\n@partial(\n    create_component_from_func,\n    packages_to_install=["dill", "pandas", "scikit-learn", "mlflow", "boto3"],\n)\ndef upload_sklearn_model_to_mlflow(\n    model_name: str,\n    model_path: InputPath("dill"),\n    input_example_path: InputPath("dill"),\n    signature_path: InputPath("dill"),\n    conda_env_path: InputPath("dill"),\n):\n    import os\n    import dill\n    from mlflow.sklearn import save_model\n    \n    from mlflow.tracking.client import MlflowClient\n\n    os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://minio-service.kubeflow.svc:9000"\n    os.environ["AWS_ACCESS_KEY_ID"] = "minio"\n    os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"\n\n    client = MlflowClient("http://mlflow-server-service.mlflow-system.svc:5000")\n\n    with open(model_path, mode="rb") as file_reader:\n        clf = dill.load(file_reader)\n\n    with open(input_example_path, "rb") as file_reader:\n        input_example = dill.load(file_reader)\n\n    with open(signature_path, "rb") as file_reader:\n        signature = dill.load(file_reader)\n\n    with open(conda_env_path, "rb") as file_reader:\n        conda_env = dill.load(file_reader)\n\n    save_model(\n        sk_model=clf,\n        path=model_name,\n        serialization_format="cloudpickle",\n        conda_env=conda_env,\n        signature=signature,\n        input_example=input_example,\n    )\n    run = client.create_run(experiment_id="0")\n    client.log_artifact(run.info.run_id, model_name)\n\n\n@pipeline(name="mlflow_pipeline")\ndef mlflow_pipeline(kernel: str, model_name: str):\n    iris_data = load_iris_data()\n    model = train_from_csv(\n        train_data=iris_data.outputs["data"],\n        train_target=iris_data.outputs["target"],\n        kernel=kernel,\n    )\n    _ = upload_sklearn_model_to_mlflow(\n        model_name=model_name,\n        model=model.outputs["model"],\n        input_example=model.outputs["input_example"],\n        signature=model.outputs["signature"],\n        conda_env=model.outputs["conda_env"],\n    )\n\n\nif __name__ == "__main__":\n    kfp.compiler.Compiler().compile(mlflow_pipeline, "mlflow_pipeline.yaml")\n')),(0,r.kt)("p",null,(0,r.kt)("details",null,(0,r.kt)("summary",null,"mlflow_pipeline.yaml"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: mlflow-pipeline-\n  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.10, pipelines.kubeflow.org/pipeline_compilation_time: \'2022-01-19T14:14:11.999807\',\n    pipelines.kubeflow.org/pipeline_spec: \'{"inputs": [{"name": "kernel", "type":\n      "String"}, {"name": "model_name", "type": "String"}], "name": "mlflow_pipeline"}\'}\n  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.10}\nspec:\n  entrypoint: mlflow-pipeline\n  templates:\n  - name: load-iris-data\n    container:\n      args: [--data, /tmp/outputs/data/data, --target, /tmp/outputs/target/data]\n      command:\n      - sh\n      - -c\n      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n        \'pandas\' \'scikit-learn\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip\n        install --quiet --no-warn-script-location \'pandas\' \'scikit-learn\' --user)\n        && "$0" "$@"\n      - sh\n      - -ec\n      - |\n        program_path=$(mktemp)\n        printf "%s" "$0" > "$program_path"\n        python3 -u "$program_path" "$@"\n      - |\n        def _make_parent_dirs_and_return_path(file_path: str):\n            import os\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            return file_path\n\n        def load_iris_data(\n            data_path,\n            target_path,\n        ):\n            import pandas as pd\n            from sklearn.datasets import load_iris\n\n            iris = load_iris()\n\n            data = pd.DataFrame(iris["data"], columns=iris["feature_names"])\n            target = pd.DataFrame(iris["target"], columns=["target"])\n\n            data.to_csv(data_path, index=False)\n            target.to_csv(target_path, index=False)\n\n        import argparse\n        _parser = argparse.ArgumentParser(prog=\'Load iris data\', description=\'\')\n        _parser.add_argument("--data", dest="data_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--target", dest="target_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n        _parsed_args = vars(_parser.parse_args())\n\n        _outputs = load_iris_data(**_parsed_args)\n      image: python:3.7\n    outputs:\n      artifacts:\n      - {name: load-iris-data-data, path: /tmp/outputs/data/data}\n      - {name: load-iris-data-target, path: /tmp/outputs/target/data}\n    metadata:\n      labels:\n        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10\n        pipelines.kubeflow.org/pipeline-sdk-type: kfp\n        pipelines.kubeflow.org/enable_caching: "true"\n      annotations: {pipelines.kubeflow.org/component_spec: \'{"implementation": {"container":\n          {"args": ["--data", {"outputPath": "data"}, "--target", {"outputPath": "target"}],\n          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip\n          install --quiet --no-warn-script-location \'\'pandas\'\' \'\'scikit-learn\'\' ||\n          PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n          \'\'pandas\'\' \'\'scikit-learn\'\' --user) && \\"$0\\" \\"$@\\"", "sh", "-ec", "program_path=$(mktemp)\\nprintf\n          \\"%s\\" \\"$0\\" > \\"$program_path\\"\\npython3 -u \\"$program_path\\" \\"$@\\"\\n",\n          "def _make_parent_dirs_and_return_path(file_path: str):\\n    import os\\n    os.makedirs(os.path.dirname(file_path),\n          exist_ok=True)\\n    return file_path\\n\\ndef load_iris_data(\\n    data_path,\\n    target_path,\\n):\\n    import\n          pandas as pd\\n    from sklearn.datasets import load_iris\\n\\n    iris = load_iris()\\n\\n    data\n          = pd.DataFrame(iris[\\"data\\"], columns=iris[\\"feature_names\\"])\\n    target\n          = pd.DataFrame(iris[\\"target\\"], columns=[\\"target\\"])\\n\\n    data.to_csv(data_path,\n          index=False)\\n    target.to_csv(target_path, index=False)\\n\\nimport argparse\\n_parser\n          = argparse.ArgumentParser(prog=\'\'Load iris data\'\', description=\'\'\'\')\\n_parser.add_argument(\\"--data\\",\n          dest=\\"data_path\\", type=_make_parent_dirs_and_return_path, required=True,\n          default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--target\\", dest=\\"target_path\\",\n          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args\n          = vars(_parser.parse_args())\\n\\n_outputs = load_iris_data(**_parsed_args)\\n"],\n          "image": "python:3.7"}}, "name": "Load iris data", "outputs": [{"name":\n          "data", "type": "csv"}, {"name": "target", "type": "csv"}]}\', pipelines.kubeflow.org/component_ref: \'{}\'}\n  - name: mlflow-pipeline\n    inputs:\n      parameters:\n      - {name: kernel}\n      - {name: model_name}\n    dag:\n      tasks:\n      - {name: load-iris-data, template: load-iris-data}\n      - name: train-from-csv\n        template: train-from-csv\n        dependencies: [load-iris-data]\n        arguments:\n          parameters:\n          - {name: kernel, value: \'{{inputs.parameters.kernel}}\'}\n          artifacts:\n          - {name: load-iris-data-data, from: \'{{tasks.load-iris-data.outputs.artifacts.load-iris-data-data}}\'}\n          - {name: load-iris-data-target, from: \'{{tasks.load-iris-data.outputs.artifacts.load-iris-data-target}}\'}\n      - name: upload-sklearn-model-to-mlflow\n        template: upload-sklearn-model-to-mlflow\n        dependencies: [train-from-csv]\n        arguments:\n          parameters:\n          - {name: model_name, value: \'{{inputs.parameters.model_name}}\'}\n          artifacts:\n          - {name: train-from-csv-conda_env, from: \'{{tasks.train-from-csv.outputs.artifacts.train-from-csv-conda_env}}\'}\n          - {name: train-from-csv-input_example, from: \'{{tasks.train-from-csv.outputs.artifacts.train-from-csv-input_example}}\'}\n          - {name: train-from-csv-model, from: \'{{tasks.train-from-csv.outputs.artifacts.train-from-csv-model}}\'}\n          - {name: train-from-csv-signature, from: \'{{tasks.train-from-csv.outputs.artifacts.train-from-csv-signature}}\'}\n  - name: train-from-csv\n    container:\n      args: [--train-data, /tmp/inputs/train_data/data, --train-target, /tmp/inputs/train_target/data,\n        --kernel, \'{{inputs.parameters.kernel}}\', --model, /tmp/outputs/model/data,\n        --input-example, /tmp/outputs/input_example/data, --signature, /tmp/outputs/signature/data,\n        --conda-env, /tmp/outputs/conda_env/data]\n      command:\n      - sh\n      - -c\n      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n        \'dill\' \'pandas\' \'scikit-learn\' \'mlflow\' || PIP_DISABLE_PIP_VERSION_CHECK=1\n        python3 -m pip install --quiet --no-warn-script-location \'dill\' \'pandas\' \'scikit-learn\'\n        \'mlflow\' --user) && "$0" "$@"\n      - sh\n      - -ec\n      - |\n        program_path=$(mktemp)\n        printf "%s" "$0" > "$program_path"\n        python3 -u "$program_path" "$@"\n      - |\n        def _make_parent_dirs_and_return_path(file_path: str):\n            import os\n            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n            return file_path\n\n        def train_from_csv(\n            train_data_path,\n            train_target_path,\n            model_path,\n            input_example_path,\n            signature_path,\n            conda_env_path,\n            kernel,\n        ):\n            import dill\n            import pandas as pd\n            from sklearn.svm import SVC\n\n            from mlflow.models.signature import infer_signature\n            from mlflow.utils.environment import _mlflow_conda_env\n\n            train_data = pd.read_csv(train_data_path)\n            train_target = pd.read_csv(train_target_path)\n\n            clf = SVC(kernel=kernel)\n            clf.fit(train_data, train_target)\n\n            with open(model_path, mode="wb") as file_writer:\n                dill.dump(clf, file_writer)\n\n            input_example = train_data.sample(1)\n            with open(input_example_path, "wb") as file_writer:\n                dill.dump(input_example, file_writer)\n\n            signature = infer_signature(train_data, clf.predict(train_data))\n            with open(signature_path, "wb") as file_writer:\n                dill.dump(signature, file_writer)\n\n            conda_env = _mlflow_conda_env(\n                additional_pip_deps=["dill", "pandas", "scikit-learn"]\n            )\n            with open(conda_env_path, "wb") as file_writer:\n                dill.dump(conda_env, file_writer)\n\n        import argparse\n        _parser = argparse.ArgumentParser(prog=\'Train from csv\', description=\'\')\n        _parser.add_argument("--train-data", dest="train_data_path", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--train-target", dest="train_target_path", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--kernel", dest="kernel", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--input-example", dest="input_example_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--signature", dest="signature_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--conda-env", dest="conda_env_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n        _parsed_args = vars(_parser.parse_args())\n\n        _outputs = train_from_csv(**_parsed_args)\n      image: python:3.7\n    inputs:\n      parameters:\n      - {name: kernel}\n      artifacts:\n      - {name: load-iris-data-data, path: /tmp/inputs/train_data/data}\n      - {name: load-iris-data-target, path: /tmp/inputs/train_target/data}\n    outputs:\n      artifacts:\n      - {name: train-from-csv-conda_env, path: /tmp/outputs/conda_env/data}\n      - {name: train-from-csv-input_example, path: /tmp/outputs/input_example/data}\n      - {name: train-from-csv-model, path: /tmp/outputs/model/data}\n      - {name: train-from-csv-signature, path: /tmp/outputs/signature/data}\n    metadata:\n      labels:\n        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10\n        pipelines.kubeflow.org/pipeline-sdk-type: kfp\n        pipelines.kubeflow.org/enable_caching: "true"\n      annotations: {pipelines.kubeflow.org/component_spec: \'{"implementation": {"container":\n          {"args": ["--train-data", {"inputPath": "train_data"}, "--train-target",\n          {"inputPath": "train_target"}, "--kernel", {"inputValue": "kernel"}, "--model",\n          {"outputPath": "model"}, "--input-example", {"outputPath": "input_example"},\n          "--signature", {"outputPath": "signature"}, "--conda-env", {"outputPath":\n          "conda_env"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1\n          python3 -m pip install --quiet --no-warn-script-location \'\'dill\'\' \'\'pandas\'\'\n          \'\'scikit-learn\'\' \'\'mlflow\'\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m\n          pip install --quiet --no-warn-script-location \'\'dill\'\' \'\'pandas\'\' \'\'scikit-learn\'\'\n          \'\'mlflow\'\' --user) && \\"$0\\" \\"$@\\"", "sh", "-ec", "program_path=$(mktemp)\\nprintf\n          \\"%s\\" \\"$0\\" > \\"$program_path\\"\\npython3 -u \\"$program_path\\" \\"$@\\"\\n",\n          "def _make_parent_dirs_and_return_path(file_path: str):\\n    import os\\n    os.makedirs(os.path.dirname(file_path),\n          exist_ok=True)\\n    return file_path\\n\\ndef train_from_csv(\\n    train_data_path,\\n    train_target_path,\\n    model_path,\\n    input_example_path,\\n    signature_path,\\n    conda_env_path,\\n    kernel,\\n):\\n    import\n          dill\\n    import pandas as pd\\n    from sklearn.svm import SVC\\n\\n    from\n          mlflow.models.signature import infer_signature\\n    from mlflow.utils.environment\n          import _mlflow_conda_env\\n\\n    train_data = pd.read_csv(train_data_path)\\n    train_target\n          = pd.read_csv(train_target_path)\\n\\n    clf = SVC(kernel=kernel)\\n    clf.fit(train_data,\n          train_target)\\n\\n    with open(model_path, mode=\\"wb\\") as file_writer:\\n        dill.dump(clf,\n          file_writer)\\n\\n    input_example = train_data.sample(1)\\n    with open(input_example_path,\n          \\"wb\\") as file_writer:\\n        dill.dump(input_example, file_writer)\\n\\n    signature\n          = infer_signature(train_data, clf.predict(train_data))\\n    with open(signature_path,\n          \\"wb\\") as file_writer:\\n        dill.dump(signature, file_writer)\\n\\n    conda_env\n          = _mlflow_conda_env(\\n        additional_pip_deps=[\\"dill\\", \\"pandas\\",\n          \\"scikit-learn\\"]\\n    )\\n    with open(conda_env_path, \\"wb\\") as file_writer:\\n        dill.dump(conda_env,\n          file_writer)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\'\'Train\n          from csv\'\', description=\'\'\'\')\\n_parser.add_argument(\\"--train-data\\", dest=\\"train_data_path\\",\n          type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--train-target\\",\n          dest=\\"train_target_path\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--kernel\\",\n          dest=\\"kernel\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--model\\",\n          dest=\\"model_path\\", type=_make_parent_dirs_and_return_path, required=True,\n          default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--input-example\\", dest=\\"input_example_path\\",\n          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--signature\\",\n          dest=\\"signature_path\\", type=_make_parent_dirs_and_return_path, required=True,\n          default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--conda-env\\", dest=\\"conda_env_path\\",\n          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\\n_parsed_args\n          = vars(_parser.parse_args())\\n\\n_outputs = train_from_csv(**_parsed_args)\\n"],\n          "image": "python:3.7"}}, "inputs": [{"name": "train_data", "type": "csv"},\n          {"name": "train_target", "type": "csv"}, {"name": "kernel", "type": "String"}],\n          "name": "Train from csv", "outputs": [{"name": "model", "type": "dill"},\n          {"name": "input_example", "type": "dill"}, {"name": "signature", "type":\n          "dill"}, {"name": "conda_env", "type": "dill"}]}\', pipelines.kubeflow.org/component_ref: \'{}\',\n        pipelines.kubeflow.org/arguments.parameters: \'{"kernel": "{{inputs.parameters.kernel}}"}\'}\n  - name: upload-sklearn-model-to-mlflow\n    container:\n      args: [--model-name, \'{{inputs.parameters.model_name}}\', --model, /tmp/inputs/model/data,\n        --input-example, /tmp/inputs/input_example/data, --signature, /tmp/inputs/signature/data,\n        --conda-env, /tmp/inputs/conda_env/data]\n      command:\n      - sh\n      - -c\n      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n        \'dill\' \'pandas\' \'scikit-learn\' \'mlflow\' \'boto3\' || PIP_DISABLE_PIP_VERSION_CHECK=1\n        python3 -m pip install --quiet --no-warn-script-location \'dill\' \'pandas\' \'scikit-learn\'\n        \'mlflow\' \'boto3\' --user) && "$0" "$@"\n      - sh\n      - -ec\n      - |\n        program_path=$(mktemp)\n        printf "%s" "$0" > "$program_path"\n        python3 -u "$program_path" "$@"\n      - |\n        def upload_sklearn_model_to_mlflow(\n            model_name,\n            model_path,\n            input_example_path,\n            signature_path,\n            conda_env_path,\n        ):\n            import os\n            import dill\n            from mlflow.sklearn import save_model\n\n            from mlflow.tracking.client import MlflowClient\n\n            os.environ["MLFLOW_S3_ENDPOINT_URL"] = "http://minio-service.kubeflow.svc:9000"\n            os.environ["AWS_ACCESS_KEY_ID"] = "minio"\n            os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"\n\n            client = MlflowClient("http://mlflow-server-service.mlflow-system.svc:5000")\n\n            with open(model_path, mode="rb") as file_reader:\n                clf = dill.load(file_reader)\n\n            with open(input_example_path, "rb") as file_reader:\n                input_example = dill.load(file_reader)\n\n            with open(signature_path, "rb") as file_reader:\n                signature = dill.load(file_reader)\n\n            with open(conda_env_path, "rb") as file_reader:\n                conda_env = dill.load(file_reader)\n\n            save_model(\n                sk_model=clf,\n                path=model_name,\n                serialization_format="cloudpickle",\n                conda_env=conda_env,\n                signature=signature,\n                input_example=input_example,\n            )\n            run = client.create_run(experiment_id="0")\n            client.log_artifact(run.info.run_id, model_name)\n\n        import argparse\n        _parser = argparse.ArgumentParser(prog=\'Upload sklearn model to mlflow\', description=\'\')\n        _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--input-example", dest="input_example_path", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--signature", dest="signature_path", type=str, required=True, default=argparse.SUPPRESS)\n        _parser.add_argument("--conda-env", dest="conda_env_path", type=str, required=True, default=argparse.SUPPRESS)\n        _parsed_args = vars(_parser.parse_args())\n\n        _outputs = upload_sklearn_model_to_mlflow(**_parsed_args)\n      image: python:3.7\n    inputs:\n      parameters:\n      - {name: model_name}\n      artifacts:\n      - {name: train-from-csv-conda_env, path: /tmp/inputs/conda_env/data}\n      - {name: train-from-csv-input_example, path: /tmp/inputs/input_example/data}\n      - {name: train-from-csv-model, path: /tmp/inputs/model/data}\n      - {name: train-from-csv-signature, path: /tmp/inputs/signature/data}\n    metadata:\n      labels:\n        pipelines.kubeflow.org/kfp_sdk_version: 1.8.10\n        pipelines.kubeflow.org/pipeline-sdk-type: kfp\n        pipelines.kubeflow.org/enable_caching: "true"\n      annotations: {pipelines.kubeflow.org/component_spec: \'{"implementation": {"container":\n          {"args": ["--model-name", {"inputValue": "model_name"}, "--model", {"inputPath":\n          "model"}, "--input-example", {"inputPath": "input_example"}, "--signature",\n          {"inputPath": "signature"}, "--conda-env", {"inputPath": "conda_env"}],\n          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip\n          install --quiet --no-warn-script-location \'\'dill\'\' \'\'pandas\'\' \'\'scikit-learn\'\'\n          \'\'mlflow\'\' \'\'boto3\'\' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n          --quiet --no-warn-script-location \'\'dill\'\' \'\'pandas\'\' \'\'scikit-learn\'\' \'\'mlflow\'\'\n          \'\'boto3\'\' --user) && \\"$0\\" \\"$@\\"", "sh", "-ec", "program_path=$(mktemp)\\nprintf\n          \\"%s\\" \\"$0\\" > \\"$program_path\\"\\npython3 -u \\"$program_path\\" \\"$@\\"\\n",\n          "def upload_sklearn_model_to_mlflow(\\n    model_name,\\n    model_path,\\n    input_example_path,\\n    signature_path,\\n    conda_env_path,\\n):\\n    import\n          os\\n    import dill\\n    from mlflow.sklearn import save_model\\n\\n    from\n          mlflow.tracking.client import MlflowClient\\n\\n    os.environ[\\"MLFLOW_S3_ENDPOINT_URL\\"]\n          = \\"http://minio-service.kubeflow.svc:9000\\"\\n    os.environ[\\"AWS_ACCESS_KEY_ID\\"]\n          = \\"minio\\"\\n    os.environ[\\"AWS_SECRET_ACCESS_KEY\\"] = \\"minio123\\"\\n\\n    client\n          = MlflowClient(\\"http://mlflow-server-service.mlflow-system.svc:5000\\")\\n\\n    with\n          open(model_path, mode=\\"rb\\") as file_reader:\\n        clf = dill.load(file_reader)\\n\\n    with\n          open(input_example_path, \\"rb\\") as file_reader:\\n        input_example\n          = dill.load(file_reader)\\n\\n    with open(signature_path, \\"rb\\") as file_reader:\\n        signature\n          = dill.load(file_reader)\\n\\n    with open(conda_env_path, \\"rb\\") as file_reader:\\n        conda_env\n          = dill.load(file_reader)\\n\\n    save_model(\\n        sk_model=clf,\\n        path=model_name,\\n        serialization_format=\\"cloudpickle\\",\\n        conda_env=conda_env,\\n        signature=signature,\\n        input_example=input_example,\\n    )\\n    run\n          = client.create_run(experiment_id=\\"0\\")\\n    client.log_artifact(run.info.run_id,\n          model_name)\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\'\'Upload\n          sklearn model to mlflow\'\', description=\'\'\'\')\\n_parser.add_argument(\\"--model-name\\",\n          dest=\\"model_name\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--model\\",\n          dest=\\"model_path\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--input-example\\",\n          dest=\\"input_example_path\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--signature\\",\n          dest=\\"signature_path\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parser.add_argument(\\"--conda-env\\",\n          dest=\\"conda_env_path\\", type=str, required=True, default=argparse.SUPPRESS)\\n_parsed_args\n          = vars(_parser.parse_args())\\n\\n_outputs = upload_sklearn_model_to_mlflow(**_parsed_args)\\n"],\n          "image": "python:3.7"}}, "inputs": [{"name": "model_name", "type": "String"},\n          {"name": "model", "type": "dill"}, {"name": "input_example", "type": "dill"},\n          {"name": "signature", "type": "dill"}, {"name": "conda_env", "type": "dill"}],\n          "name": "Upload sklearn model to mlflow"}\', pipelines.kubeflow.org/component_ref: \'{}\',\n        pipelines.kubeflow.org/arguments.parameters: \'{"model_name": "{{inputs.parameters.model_name}}"}\'}\n  arguments:\n    parameters:\n    - {name: kernel}\n    - {name: model_name}\n  serviceAccountName: pipeline-runner\n')))),(0,r.kt)("p",null,"After generating the mlflow_pipeline.yaml file after execution, upload the pipeline and execute it to check the results of the run."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"mlflow-svc-0",src:a(5041).Z,width:"3408",height:"2156"})),(0,r.kt)("p",null,"Port-forward the mlflow service to access the MLflow UI."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward svc/mlflow-server-service -n mlflow-system 5000:5000\n")),(0,r.kt)("p",null,"Open the web browser and connect to localhost:5000. You will then be able to see that the run has been created as follows."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"mlflow-svc-1",src:a(1757).Z,width:"3360",height:"2100"})),(0,r.kt)("p",null,"Click on run to verify that the trained model file is present."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"mlflow-svc-2",src:a(1874).Z,width:"3360",height:"2100"})))}_.isMDXComponent=!0},8730:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-0-95d5ec759ef43b21c9c3b22abb64366d.png"},6694:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-1-a096f3eda2246a1c132fc13ce3180ef5.png"},5944:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-2-3cd7cf7e2c853a1242cff7c65e56cf3f.png"},5109:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-3-8b187057bb18f27b1744656ef6d045a1.png"},5041:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-svc-0-ab6c5d7f00bf643c36d236155dc5eb9c.png"},1757:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-svc-1-7723b8f92fb8cea2ff99b8f4639ff0c6.png"},1874:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/mlflow-svc-2-8b696bd65a922f949877102bbfdafc42.png"}}]);